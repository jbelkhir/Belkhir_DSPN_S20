{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) iii - For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high  enough.\n",
    "\n",
    "Given that $X_1 = GPA$, $X_2 = IQ$, $X_3 = Gender$ (1 for Female and 0 for Male), $X_4$ = Interaction between GPA and IQ, and $X_5$ = Interaction between GPA and Gender, and that '$\\hat{\\beta_0}$ =50, $\\hat{\\beta_1}$ =20, $\\hat{\\beta_2}$ =0.07, $\\hat{\\beta_3}$ =35, $\\hat{\\beta_4}$ =0.01, $\\hat{\\beta_5}$ =-10 ., we know that the interaction between GPA and Gender is negative, and this value is nonzero in the cases of females (-1), so as GPA gets higher and higher, women make less and less relative to their male counterparts(i.e. same GPA and IQ). As GPA gets lower and lower, females make more and more relative to their male counterparts (i.e. same GPA and IQ). As such, we know that for any fixed value of IQ and GPA, males will earn more on average than females given that the GPA is high enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Predict the salary of a female graduate with IQ of 110 and a GPA of 4.0.\n",
    "\n",
    "$Y$ = $\\hat{\\beta_0}$$X_1$ +  $\\hat{\\beta_1}$$X_1$ + $\\hat{\\beta_2}$$X_2$ + $\\hat{\\beta_3}$$X_3$ + $\\hat{\\beta_4}$$X_4$ + $\\hat{\\beta_5}$$X_5$\n",
    "\n",
    "$Y$ = $\\hat{\\beta_0}$ +  $\\hat{\\beta_1}$GPA + $\\hat{\\beta_2}$IQ + $\\hat{\\beta_3}$Gender + $\\hat{\\beta_4}$Interaction\n",
    "    between GPA and IQ + $\\hat{\\beta_5}$Interaction between GPA and Gender\n",
    "    \n",
    "$Y$ = $\\hat{\\beta_0}$ +  $\\hat{\\beta_1}$(GPA) + $\\hat{\\beta_2}$(IQ) + $\\hat{\\beta_3}$Gender + $\\hat{\\beta_4}$(GPA x IQ) + $\\hat{\\beta_5}$(GPA x Gender)\n",
    "\n",
    "$Y$ = $50$ +  $20$(GPA) + $.07$(IQ) + $35$(Gender) + $.01$(GPA x IQ) + $-10$(GPA x Gender)   \n",
    "\n",
    "$Y$ = $50$ +  $20$(4) + $.07$(110) + $35$(1) + $.01$(4 x 110) + $-10$(4 x 1)   \n",
    "\n",
    "$Y$ = $50$ +  $80$ + $7.70$ + $35$ + $4.40$ + $-40$ \n",
    "\n",
    "$Y$ = $137.1$ \n",
    "\n",
    "A female graduate with IQ of 110 and GPA of 4.0 is predicted to make 137.1k a year as a starting salary agter graduation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small,\n",
    "there is very little evidence of an interaction effect. Justify your answer.\n",
    "\n",
    "False, the magnitude of a coefficient for an interaction term, or any term in a linear model, does not imply anything \n",
    "to the amount of evidence for an effect or relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - High leverage points are essentially Anomalies in $X$, that have a particularly high influence on the estimation of $f(X)$, or in other words, data points that have a high Cook's distance. These kind of data points can lead to violation of the linearity and equal variace assumptions. High leverage points, are going to have stronger relative influence on the estimated regression line than the other observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a) The assumotion with $Z$ is that it has its own random structure. And as such, the variance of the regression coefficients on the random effects ($\\upsilon$) have a different variance than that of the regression coefficients on the fixed effects ($\\beta$). \n",
    "\n",
    "3b) the objective function for this model $$min(||Y-X\\beta-Z\\Lambda_{\\theta}\\upsilon||^2+||\\upsilon||^2)$$. And this is because we have to estimate an independent covariance matrix ($\\Lambda_{\\theta}$), which in turn explains the random structure in $Z$. Because we have to estimate $\\Lambda_{\\theta}$ (and by extension $\\Sigma_{\\theta}$), this differentiates the mixed effects model and from nuisance regression.\n",
    "\n",
    "3c) It allows you to incorporate categorical variables that may not be of primary interest to your hyporthesis but could have a systematic influence response variable that is critical to understanding for your overall investigation.\n",
    "\n",
    "3d) Classroom number and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also installing the dependency ‘nloptr’\n",
      "\n",
      "Warning message in install.packages(package, repos = \"http://cran.us.r-project.org\"):\n",
      "“installation of package ‘nloptr’ had non-zero exit status”Warning message in install.packages(package, repos = \"http://cran.us.r-project.org\"):\n",
      "“installation of package ‘lme4’ had non-zero exit status”Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in library(package, character.only = TRUE): there is no package called ‘lme4’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(package, character.only = TRUE): there is no package called ‘lme4’\nTraceback:\n",
      "1. import_packages(c(\"lme4\"))",
      "2. library(package, character.only = TRUE)   # at line 7 of file <text>"
     ]
    }
   ],
   "source": [
    "# Clear the workspace again\n",
    "rm(list=ls())\n",
    "\n",
    "# Install LME4\n",
    "install.packages(\"lme4\")\n",
    "install.packages(\"ISLR\")\n",
    "install.packages(\"boot\")\n",
    "install.packages(\"ggplot2\")\n",
    "library(lme4)\n",
    "library(ggplot2)\n",
    "\n",
    "?cbpp\n",
    "d<-cbpp\n",
    "plot(d$size, d$incidence)\n",
    "fit<-lm(d$incidence~d$size)\n",
    "abline(fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'cbpp' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'cbpp' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "fit<-lm(d$incidence~d$size)\n",
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me.fit1 = lmer(d$incidence ~ d$size + (d$size | d$herd)) \n",
    "summary(me.fit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will compare the two models using the Akaike information criterion (AIC)\n",
    "ic = AIC(fit, me.fit1)\n",
    "ic\n",
    "diff(ic$AIC)\n",
    "#This difference in AIC is relevant, which means that the more complex model (the mixed-effects model) accounts for  \n",
    "#more variance in incidence than the simple linear model, even after accounting for its increased complexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "✔ ggplot2 3.1.1       ✔ purrr   0.3.2  \n",
      "✔ tibble  2.1.1       ✔ dplyr   0.8.0.1\n",
      "✔ tidyr   0.8.3       ✔ stringr 1.4.0  \n",
      "✔ readr   1.3.1       ✔ forcats 0.4.0  \n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Question 5(a)\n",
    "# -------------------------------\n",
    "\n",
    "# function to install the packages \n",
    "import_packages <- function(packages) {\n",
    "  for (package in packages) {\n",
    "    if (!(package %in% row.names(installed.packages()))) { \n",
    "      install.packages(package, repos = \"http://cran.us.r-project.org\") \n",
    "    } \n",
    "    library(package, character.only = TRUE) \n",
    "  }\n",
    "}\n",
    "\n",
    "import_packages(c('tidyverse'))\n",
    "\n",
    "\n",
    "setwd(\"~/Desktop/Grad_School/Classes/Spring_2019/Classes/Data_Science/DataSciencePsychNeuro_CMU85732/Homeworks/hcp_data/\")\n",
    "d<-read.csv(\"unrestricted_trimmed_1_7_2020_10_50_44.csv\")\n",
    "d %>%\n",
    "  select( Subject, Gender, Age, FS_IntraCranial_Vol, FS_Total_GM_Vol)  %>%\n",
    "  na.omit() -> d1\n",
    "\n",
    "d1 %>%\n",
    "  mutate(IC_zscore = (FS_IntraCranial_Vol - mean(FS_IntraCranial_Vol))/sd(FS_IntraCranial_Vol)) %>% \n",
    "  mutate(GM_zscore = (FS_Total_GM_Vol - mean(FS_Total_GM_Vol))/sd(FS_Total_GM_Vol)) %>% \n",
    "  na.omit() -> d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit=glm(d1$Gender~d1$FS_IntraCranial_Vol, family=binomial)\n",
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_prob_df = data.frame(predict(fit, type = \"response\"))\n",
    "colnames(glm_prob_df) = c('predicted_prob')\n",
    "glm_prob_df$index = seq(1, nrow(glm_prob_df))\n",
    "ggplot(glm_prob_df, aes(index, predicted_prob)) + geom_point() + xlab('observation') + ylab('predicted response probability')\n",
    "contrasts(d1$Gender)\n",
    "threshold = 0.50 #binarizing threshold \n",
    "glm_prob_df$predicted_binary=rep(\"F\",1113)\n",
    "glm_prob_df$predicted_binary[glm_prob_df$predicted_prob>threshold]=\"M\"\n",
    "confusion_df = data.frame(glm_prob_df$predicted_binary, d1$Gender)\n",
    "colnames(confusion_df) = c('predicted', 'actual')\n",
    "table(confusion_df)\n",
    "mean(confusion_df$predicted == confusion_df$actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(glm.fit)\n",
    "library(boot)\n",
    "glm.fit=glm(Gender~FS_IntraCranial_Vol, data = d1, family=binomial)\n",
    "cv.err=cv.glm(d1,glm.fit, K=nrow(d1))\n",
    "cv.err$delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for polynomial models up to a factor of 10\n",
    "cv.error = rep(0,10) # Always have to specify your output object\n",
    "for (i in 1:10) {\n",
    "  glm.fit=glm(Gender~poly(FS_IntraCranial_Vol,i), data=d1, family=binomial)\n",
    "  cv.error[i] = cv.glm(d1, glm.fit)$delta[1]\n",
    "}\n",
    "cv.error\n",
    "# Reset the seed\n",
    "set.seed(17)\n",
    "\n",
    "# Repeat our previous loop  but use K-fold CV \n",
    "# where K = 10 and up to a 5th order polynomial\n",
    "cv.error.10 = rep(0,10) # Always have to specify your output object\n",
    "for (i in 1:10){\n",
    "  glm.fit=glm(Gender~poly(FS_IntraCranial_Vol,i), data=d1, family=binomial)\n",
    "  cv.error.10[i] = cv.glm(d1, glm.fit, K=10)$delta[1]\n",
    "}\n",
    "plot(cv.error, cv.error.10, xlab=\"LOOCV error\", ylab=\"10-fold CV error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "lda.fit = lda(Gender~FS_IntraCranial_Vol, data=d1)\n",
    "lda.fit \n",
    "plot(lda.fit)\n",
    "lda.pred = predict(lda.fit, d1)\n",
    "lda.class = lda.pred$class\n",
    "table(lda.class, d1$Gender)\n",
    "mean(lda.class==d1$Gender)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
